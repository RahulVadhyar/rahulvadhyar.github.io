---
title: 'Give me attention: LLMs and Transformers'
date: 2023-05-19
categories: [Machine Learning]
tags: [attention, transformers, sequential, LLM]     # TAG names should always be lowercase
description: A detailed explaination of attention mechanism and how transformers work.
comments: true
math: true
toc: true
---

# Insert a PIC here

# Introduction

# What we are trying to do

# What we did before

# What is attention

# Attention Explained

# What makes this better

# Transformer Architecture
some markdown

# Conclusion